18301137 王磊的工作完成情况以及计划：

2020.06.30 上午
    完成了配置python、pycharm、java以及spark环境的相关操作，并且对github的管理更加明确，建立了worklog目录并写明了目录说明，来告知成员们怎样更新自己的日志文件。
    由于一位成员的错误操作，导致之前添加的目录等文件在github和本地库中均被删除，我在网上查找了很多相关资料，明白了用什么语句来找到之前的版本并找回被删除的文件。
虽然看到文件被删除后很崩溃，但是当学会如何恢复被删除的文件后，也觉得这是一件塞翁失马，焉知非福的事情。
    因为之后项目中会用到spark和Flask，因此对这两种技术的原理与运用有了一些了解，为之后的项目开发做了一些铺垫。

2020.06.30 下午
    昨天下载了坚果VPN作为翻墙工具，但只能免费1天，而且之后每个月收费32元，因此询问了同学别的便宜的VPN软件。在www.maoni.com网站上花10块钱买了一个VPN，但是无奈
按照教程重复了许多遍，改变了很多设置，也询问了同学，这个软件还是无法应用，白白亏损了10元。所以暂时先使用坚果VPN翻墙，等到有空时再继续钻研www.maoni.com网站。
    另外，完成了spark相关的环境配置，在NOAA网站获取了北京1980/1/1到2020/6/27日的每日最高气温、最低气温、平均气温以及降雨量等数据，但是由于网站的问题，点击阳光
和水时，网站会出错，因此没有下载阳光、水的相关数据，尚不明确这是否会对之后的数据分析产生影响，若有影响，之后可考虑其他方法将所有数据下载完成。
    然后，在任务7安装依赖包时，安装Statsmodels、Pandas、Numpy时操作正常，但是当安装Matplotlib时显示无法找到对应版本，所以选择了更新pip到最新版本，然后再下载
Matplotlib则成功。通过这次的环境配置，我觉得环境配置也是工程中的一个难点，是十分令人头疼的。
    目前环境还差虚拟机和spark集群的配置，这两个配置将在今晚和明早尽快完成，计划明天完成spark数据清洗与ARIMA模型研究。

2020.07.01
    今天上午首先进行了虚拟机配置的尝试，下载完Vmware在安装时，发现硬盘空间不够，原因是C盘剩余空间大小不够。然后又仔细阅读了一遍文档，发现配置虚拟机主要是为了对
数据进行清洗，即挑选出我们需要的完整数据，因此这项工作不需要所有的组员都完成，只要有一个人完成了数据清洗，便可以将这个数据导出给其他组员，并且数据清洗还可以使用
Windows版本的，配置虚拟机的目的主要是对叫大批量的数据进行操作，但是我们决定先完成北京的数据，因此决定先不配置虚拟机以及spark集群，当后面进行大批量数据操作时再
进行配置。
    之后，我便一直在研究ARIMA模型，查找了很多博客和网站，看到了很多人对于ARIMA模型的理解，目前学到了一些关于该模型的知识，也尝试了对python代码的模仿与编写，但是
目前只是生成了北京气温数据的曲线图，后面的对数据的差分，求自相关函数、偏自相关函数等的代码还未成功实现，主要是因为之前接触的python很少，对python代码很不熟悉，因此
明天将对python进行相关学习。通过看博客，我现在知道了AR和MA模型的概念，也知道了预测数据的流程，但是某些具体细节还没有看懂。
    明日计划：继续研究ARIMA模型以及python语法，争取可以在明天完成一遍整个气温预测的流程，位置后大批量数据的预测做铺垫。
