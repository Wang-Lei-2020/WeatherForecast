18301137 王磊的工作完成情况以及计划：

2020.06.30 上午
    完成了配置python、pycharm、java以及spark环境的相关操作，并且对github的管理更加明确，建立了worklog目录并写明了目录说明，来告知成员们怎样更新自己的日志文件。
    由于一位成员的错误操作，导致之前添加的目录等文件在github和本地库中均被删除，我在网上查找了很多相关资料，明白了用什么语句来找到之前的版本并找回被删除的文件。
虽然看到文件被删除后很崩溃，但是当学会如何恢复被删除的文件后，也觉得这是一件塞翁失马，焉知非福的事情。
    因为之后项目中会用到spark和Flask，因此对这两种技术的原理与运用有了一些了解，为之后的项目开发做了一些铺垫。

2020.06.30 下午
    昨天下载了坚果VPN作为翻墙工具，但只能免费1天，而且之后每个月收费32元，因此询问了同学别的便宜的VPN软件。在www.maoni.com网站上花10块钱买了一个VPN，但是无奈
按照教程重复了许多遍，改变了很多设置，也询问了同学，这个软件还是无法应用，白白亏损了10元。所以暂时先使用坚果VPN翻墙，等到有空时再继续钻研www.maoni.com网站。
    另外，完成了spark相关的环境配置，在NOAA网站获取了北京1980/1/1到2020/6/27日的每日最高气温、最低气温、平均气温以及降雨量等数据，但是由于网站的问题，点击阳光
和水时，网站会出错，因此没有下载阳光、水的相关数据，尚不明确这是否会对之后的数据分析产生影响，若有影响，之后可考虑其他方法将所有数据下载完成。
    然后，在任务7安装依赖包时，安装Statsmodels、Pandas、Numpy时操作正常，但是当安装Matplotlib时显示无法找到对应版本，所以选择了更新pip到最新版本，然后再下载
Matplotlib则成功。通过这次的环境配置，我觉得环境配置也是工程中的一个难点，是十分令人头疼的。
    目前环境还差虚拟机和spark集群的配置，这两个配置将在今晚和明早尽快完成，计划明天完成spark数据清洗与ARIMA模型研究。

2020.07.01
    今天上午首先进行了虚拟机配置的尝试，下载完Vmware在安装时，发现硬盘空间不够，原因是C盘剩余空间大小不够。然后又仔细阅读了一遍文档，发现配置虚拟机主要是为了对
数据进行清洗，即挑选出我们需要的完整数据，因此这项工作不需要所有的组员都完成，只要有一个人完成了数据清洗，便可以将这个数据导出给其他组员，并且数据清洗还可以使用
Windows版本的，配置虚拟机的目的主要是对叫大批量的数据进行操作，但是我们决定先完成北京的数据，因此决定先不配置虚拟机以及spark集群，当后面进行大批量数据操作时再
进行配置。
    之后，我便一直在研究ARIMA模型，查找了很多博客和网站，看到了很多人对于ARIMA模型的理解，目前学到了一些关于该模型的知识，也尝试了对python代码的模仿与编写，但是
目前只是生成了北京气温数据的曲线图，后面的对数据的差分，求自相关函数、偏自相关函数等的代码还未成功实现，主要是因为之前接触的python很少，对python代码很不熟悉，因此
明天将对python进行相关学习。通过看博客，我现在知道了AR和MA模型的概念，也知道了预测数据的流程，但是某些具体细节还没有看懂。
    明日计划：继续研究ARIMA模型以及python语法，争取可以在明天完成一遍整个气温预测的流程，为之后大批量数据的预测做铺垫。

2020.07.02
    今天一天都在看CSDN上对ARIMA和ARMA模型的讲解与代码示范，与昨天相比，今天对ARIMA的了解更深入了，基本对AR、MA、ARMA、ARIMA模型都有了一定的认识，并知道了模型参数
中p、d、q的含义以及参数的计算方法。目前了解到的参数计算方法主要有三种：（i）ACF和PACF定阶：通过自相关函数和偏自相关函数，画出两个图像，然后人为分别读取出q和p的值，
但是这种方法不适合普遍情况，因为无法通过计算机自身来获取参数的值(ii)信息准则定阶：包括AIC、BIC、HQ准则等，这种方法的思路一般是遍历所有可能的p、q、d的取值，然后对
所有这些情况计算相应的值，选取值最小的一种参数组合作为模型的参数。此方法适合计算机执行，并且今天已经用代码测试了BIC准则，效果还不错。(iii)热力图定阶:此方法和(ii)
类似，只不过值的表达是应用的热力图的方式，使得结果更加直观，人们更容易读取出p、q的取值，但是对于计算机来说，这种方法是不太合适的。除此之外，还进行了平稳序列的检测，
以及通过差分法获取平稳序列、白噪声检测、对模型效果的分析等等，这些操作均通过代码进行了实现。
    到此为止，今天已经完成了昨日的计划，用代码完整的实现了对北京6月28日气温的预测，并对python代码以及语法有了更多地了解，基本可以看懂博客上关于模型的相关代码，但是
对某些python语句还是不理解，在之后还需要继续对python进行熟悉。目前对ARIMA模型的理解肯定还有不太好的地方，但暂时先放下对模型的研究，继续完成后面的工作，等之后项目进度
完成的较好时，再对ARIMA模型进行更深入的研究，争取将ARIMA模型研究透。
    明日计划：创建一个python处理类，写一个标准化的处理程序，对不同日期的数据进行预测，并对预测后的数据进行处理，生成标准化的JSON数据，若时间仍充裕，就了解一下数据
传输方面的知识，学会如何将JSON数据传输出去。
    
2020.07.03
    今天上午将昨天写的ARIMA模型的代码整合了一下，完整的写了一个ARIMA模型预测数据的代码，但在p、q参数的求解上还有一些不理解，查阅了很多资料，只是大概有了一个了解，
但是在具体的实现上，网络上也没有给出一个很好的例子，因此目前只是大概确定了一个p、q参数，先在这个参数的基础上完成后面的过程，最后有时间再返回来研究参数的优化。之后
根据写的ARIMA模型的代码，编写了一个python处理类，用于其他文件对ARIMA预测的调用，封装成了一个类和相应的函数，其他文件需要用模型预测时仅需要新建一个类并调用函数即可。
然后通过在网上查阅相关资料，完成了将处理后的数据转换为Json格式的数据，生成了标准化的Json数据格式，并也将这部分代码封装成了函数，便于其他文件的调用。然后，便开始了
对Flask的研究，用pip完成了Flask的安装配置，也成功通过几行代码完成了最简单的一个demo，通过http://192.168.10.7:5023/（ip+端口）网址进行了访问，是一个空白的网页。
目前，正在研究Flask的路由配置，还没有开始写这一部分的代码。
    我会在一会将今天所写的python文件上传到我的分支——wl分支。
    明日计划：完成Flask的路由配置及相关代码的编写，然后对socket数据传输进行研究，有时间的话在看看微服务架构设计的相关知识。
    
2020.07.04
    今天，首先查找了关于Flask的资料，了解了Flask的作用，然后对Flask的路由配置进行了研究，明白了通过@app.route("/")语句来控住路由，当在URL中输入这个路由时，便会调用
这个路由下编写的函数。另外也查看了动态URL的相关资料，实现了一个简单的Demo，是用@app.route('/user/<username>')语句完成的，思想就是在URL中随机输入一个username，这个
username便相当于一个参数传给服务器，然后在页面中将参数显示出来。
    然后，为了完成Json数据传输便开始学习了socket的相关知识，在网上查阅了资料，对socket的思想和实现有了一些认识，便开始了数据传输代码的编写，目前已将代码完成并上传到
了wl分支，但是还没有对这个文件进行测试，尚不清楚时候能正确的将数据传输出去。之后，看了一些微服务架构设计的知识，但是还没有理解其内容，也不清楚如何用Java调用python的
接口，明天首先将这些工作完成，然后开始与负责前端的同学进行交接，尽量能够将前后端实现简单的连接。
    今天写的代码已经上传到了wl分支。

2020.07.05
    今天的进度相比于前几天可以说是最慢的一天了，也是最烦躁的一天。。。
    上午，我查找了关于用Flask建立微服务的相关资料，看到了很多解释，也看到了其他人写的代码示例，但始终还是不太理解这个微服务怎么用的。在这个地方卡主之后，便开始继续
往下看任务，接下来就是和Java语言相关的内容，因为前段有专门的同学来负责，所以就开始了与负责前端的同学的讨论，讨论了一番，还是对整个网络连接有不清楚的地方，不了解到底
是怎样的一个过程。
    下午，继续上午的工作，查找如何将python与java实现接口对接，即如何将用flask写的程序与用java写的前端程序构成网络连接，经过一番查找，了解到flask可以创建一个网站，
类似于http：//127.0.0.1:9000/这种格式，而前端正是用Websocket与这个类似网站的URL进行连接，到这里，我们才对整个任务流程有了一些理解，也大概明白了python程序是如何与
java程序对接的。但是由于时间有限，目前还没能达到两部分的连接，争取在明天搞定这部分的内容。另外，有组员提到如何预测所有日期的气温，要是预测所有日期气温的话，也需要
每个日期的数据，所以之前获取的北京每年6.28的气温数据就不够用了，然后便研究如何将全部日期的数据提取出来，经过询问其他组的同学后，得知只需把数据清洗中的query部分代码
删除即可。然后再用其他日期的数据来训练模型时，发现之前写的python处理类出现了error，错误是SVD不收敛，经过查询，发现是p、q参数设置不合理，知道这个之后我便很烦躁，这
就意味着我们之前写的模型有错误，便开始对身边其他同学的询问，问了很多同学后，得知他们也遇到了这种问题，并且有不少小组已经抛弃了ARIMA模型，改用其他的模型。在经过小组
内部的讨论之后，我们也决定更换模型，改用auto_arima模型，目前正在对这个模型进行研究，写了一些代码，但还没完全成功，争取在明天完成模型的改变。
    明日任务：完成模型的改进，争取与前端同学建立连接。

2020.07.06
    今天也是极其崩溃的一天，截止到下午五点，也就是刚刚，才终于将python处理类中的ARIMA模型成功改成了auto_arima模型，虽然改动的代码比较少，但对于我来说这确实是一个飞跃
从昨天晚上11点开始改，到凌晨两点都没能改对，今天下午两点，问了问其他组的同学是怎样改的，结果在我这里也不能实现，然后自己又尝试了再尝试，终于将模型改进成功，不仅可以较
好地预测出气温数据，还可以与之后的数据处理的格式统一，其实卡住的地方就是数据格式不统一的问题，不过幸好，他终于能够成功运行了。
    我也知道我们组的进度比较慢，所以今天上午也是一直在听其他组和老师们的讲解，有的组已经基本将任务完成，这时我感到了很大的压力，因此较难的Websocket问题还没能攻破，这
也是我今天晚上将要研究的问题，抓紧实现与前端同学的连接，如果晚上有进展，今晚会继续更新log。

    模型更改以及通过参数确定预测日期的代码已上传到wl分支
    
    在flask里添加了预测生成一周连续七天的函数，然后将这七天的气温数据传给前端。由于每次跑模型都需要几秒时间，因此会对用户造成很差的体验，所以暂时决定将几个城市的所有数据
提前预测出来，之后直接调用即可。

2020.07.07
    今天上午，和组员一起商量要怎样处理老师给的大量问题，因为数据下载下来后，每年每个地方的数据都是分开的，而且只有打开csv文件后才能直到文件对应的是哪个城市的数据，因此
讨论后决定，不再使用老师给的那100多G数据，而是自己去NOAA官网上下载几个城市的历史数据，然后分别对每个城市每天的气温数据做预测，将结果记录到csv文件中，这样前端调用的时候
便不再需要等待跑模型的时间了，可以增强用户的体验感。
    然后，还是继续了昨天的工作，看了一些Websocket的连接方式，但是仍然没有看的很懂，由于在这个地方已经消耗了大量时间，所以决定不再研究websocket，转为用socket连接，传输
预测的气温数据，经过测试与改进，目前已经可以达成java与python的连接，数据也可以传输，但是还没能完成传输七天的气温数据，今天晚上将会继续写这一功能的代码，争取今晚将网络
连接的传输数据功能实现。
    今天的代码已上传到wl分支。
来自凌晨更新：
    经过一晚上的研究，终于能够实现了java和python的网络连接，现在java可以给python传month和day两个参数，然后python把以后七天每天气温的最大最小值用json数据格式传给java，
并且我已把java那边的数据已经都转换成double格式，所以前端直接调用这些数据生成图像即可，卡了将近3天的网络连接传输数据终于完成了！！！！！！
    因为我们的目标是前端显示几个城市的气温预测，所以目前的网络连接传递的参数还少了一个城市的参数city，这个参数我会在明天加到java和python的函数中。另外，明天也会和前端的
同学完成合并，使我写的java代码可以加到前端上并正常运行。然后，就可以给负责前端的同学分担一些任务，首先配置SpringMVC，然后完成角色管理等方面的代码。
    今晚的代码已上传到wl分支。
